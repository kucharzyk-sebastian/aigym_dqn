{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# <center>Agent DQN doszkalany ewolucyjnie</center>\n",
    "<center><img src=\"https://media.giphy.com/media/dyde6O8yn4oRh7R1Vk/source.gif\" width=\"460\" height=\"280\" /></center>\n",
    "\n",
    "*<center>Wykonali</center>*\n",
    "*<center>Jakub Gros, Sebastian Kucharzyk</center>*\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cel i Analiza Projektu\n",
    "<p style='text-align: justify;'>\n",
    "Jako cel obraliśmy sobie stworzenie maszynowego odpowiednika człowieka grającego w grę \n",
    "komputerową(tzw. agenta), który ucząc się na własnych błędach, będzie osiągał w niej coraz to lepsze \n",
    "rezultaty. Do tego celu nadaje się paradygmat procesu uczenia nazywany uczeniem ze wzmocnieniem,\n",
    "polegający na umieszczeniu uczącego się agenta w nieznanym mu dotąd środowisku i pozwoleniu na \n",
    "wykonywanie dowolnych interakcji z otoczeniem. Agent w trakcie eksploracji doświadcza wielu różnych \n",
    "sytuacji zwanych stanami. Znajdując się w danym stanie wybiera, którą z obecnie możliwych akcji \n",
    "wykona, za co  finalnie otrzymuje pewną nagrodę lub karę. Wszystkie informacje na temat ruchów i \n",
    "nagród przechowuje w pamięci, aby nastepnie na ich podstawie móc oceniać, jaka akcja da mu \n",
    "największą nagrodę. Pozornie wydawałoby się, że rozwiązanie to nadaje się niemal idealnie do sytuacji \n",
    "wymagających, aby agent sam zebrał dane, na których następnie będzie się uczył, ale niestety posiada ono \n",
    "jedną znaczącą wadę - wraz z upływem czasu ilość danych w pamięci agenta rośnie, co z kolei powoduje \n",
    "spowolnienie podejmowania decyzji, a finalnie może nawet prowadzić do zapełnienia pamięci \n",
    "fizycznej urządzenia.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Aby uniknąć problemów związanych z nadmiarem danych, zdecydowaliśmny się na wykorzystanie innego paradygmatu, \n",
    "który pozwala zgeneralizować zbiór danych zamiast nieskończenie go poszerzać i przeszukiwać. Dokładniej \n",
    "rzecz biorąc, wybór padł na zastosowanie głębokiego Q-learningu, czyli połączenia uczenia \n",
    "ze wzmocnieniem z uczeniem głębokim. Polega on na zmniejszeniu nieograniczonej pamięci agenta \n",
    "do relatywnie małego bufora pamiętającego N ostatnich wyborów, który po każdej akcji wykonannej \n",
    "przez agenta jest wykorzystywany do  iteracyjnego dotrenowywania sieci neuronowej. Wykonuje się to za \n",
    "pomocą Q-wartości, czyli pewnej funkcji  <b>Q(S, A)</b> szacującej jak bardzo opłąca się podjąć akcję A \n",
    "będąc w stanie S. Cały powyższy proces prowadzi do stworzenia modelu Deep Q-Network(DQN).\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Niemniej jednak nie chcieliśmy ograniczać się jedynie do uczenia głębokiego, więc postanowiliśmy\n",
    "znaleźć zastosowanie dla innej dziedziny z zakresu stucznej inteligencji - algorytmów ewolucyjnych. \n",
    "Patrząc na sposób działania modelu DQN można spostrzec trzy obszary, które potencjalnie nadają się \n",
    "do ulepszenia ewolucyjnego:\n",
    "</p>\n",
    "\n",
    "1. Kształt sieci neuronowej\n",
    "2. Wagi połączeń międzyneuronowych\n",
    "3. Dane przechowywane w pamięci agentów\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Po dokładnym przeanalizowaniu powyższych możliwości zdecydowaliśmy o odrzuceniu opcji numer 1 oraz 3. \n",
    "O wyeliminowaniu manipulacji kształtem sieci neuronowej zadecydował fakt, że ... . Natomiast jeśli \n",
    "chodzi o trzecią opcję, to jej odrzucenie wynika z natury DQN, której agent przechowuje w pamięci \n",
    "jedynie mały  fragment z całego zbioru danych, na którym był uczony, więc używanie go jako podstawę\n",
    "do dalszego ulepszania najprawdopodobniej nie porwadziłoby do uzyskania wymiernych korzyści. \n",
    "W taki sposób stwierdziliśmy, że najciekawszym i(być może) najefektywniejszym podejściem będzie \n",
    "zastosowanie algorytmu genetycznego operującego na wagach połączeń międzyneuronowych.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ograniczenia sprzętowe\n",
    "<p style='text-align: justify;'>\n",
    "W momencie gdy zaimplementowaliśmy prostego i stosunkowo uniwersalnego agenta, który mógł być \n",
    "trenowany z wykorzystaniem większości środowisk dostępnych poprzez interfejs AI Gym, okazało się, \n",
    "że nasz sprzęt nie jest wystarczająco wydajny, aby zrealizować ten projekt w akceptowalnych ramach \n",
    "czasowych. Mówiąc konkretniej, trenowanie naszego modelu do stopnia, w którym jego wyniki są \n",
    "powtarzalne i można zacząć wyciągać z nich wnioski, w najprostszym środowisku(CartPole) zajmuje \n",
    "około 10 minut, natomiast w drugim co do złożoności LunarLander czas ten wzrasta aż do 45 minut. \n",
    "Po oszacowaniu, że do zakończenia projektu będziemy musieli trenować model w różnych konfiguracjach \n",
    "blisko 100 razy, liczba 75 godzin spędzona jednynie na trenowaniu LunarLandera byłaby zbyt duża.\n",
    "</p>\n",
    "\n",
    "## Zastosowane technologie\n",
    "<p style='text-align: justify;'>\n",
    "Kod projektu napisaliśmy w języku Python w wersji 3.6, z którego znakomita większość wykonuje się \n",
    "na procesorze. Karta graficzna jest odpowiedzialna za przeprowadzanie operacji czerpiących wymierne \n",
    "korzyści z wielowątkowości, a więc za trenowanie sieci neuronowej. Sama sieć oraz wszelkie operacje \n",
    "z nią związane pochodzą z wysokopoziomowego API Kerasa bazującego na bibliotece TensorFlow. Jeśli \n",
    "zaś chodzi o środowisko, w którym odbywa się trenowanie, to jest to CartPole z biblioteki AI Gym.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Celem projektu jest stworzenie i wytrenowanie agenta DQN w taki sposób, aby osiągał jak najlepsze rezultaty w grze\n",
    "typu Cart Pole.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Etapy działania\n",
    "\n",
    "Projekt można podzielić na następujące fazy:\n",
    "1. Implementacja naiwnego agenta zawsze podejmującego decyzję w sposób losowy\n",
    "2. Implementacja agenta wykorzystującego prostą sieć neuronową\n",
    "3. Poprawienie rezultatów osiąganych przez agenta poprzez zmiany w jego implementacji\n",
    "4. Zebranie pamięci z pięciu agentów, a następnie ewolucyjne mieszanie ich parametrów tak,\n",
    "aby otrzymać agenta z najlepszą bazą przypadków do nauki\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model losowy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "a = random.randint()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NUM_OF_EPISODES = 200\n",
    "FRAMES_PER_EPISODE = 1000\n",
    "BATCH_SIZE = 32\n",
    "GAME_ID = 'CartPole-v1'\n",
    "CPU_ID = '/device:CPU:0'\n",
    "print(0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Header"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Header"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Header"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Header"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}