{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# <center>Agent DQN doszkalany ewolucyjnie</center>\n",
    "<center><img src=\"https://media.giphy.com/media/dyde6O8yn4oRh7R1Vk/source.gif\" width=\"460\" height=\"280\" /></center>\n",
    "\n",
    "*<center>Wykonali</center>*\n",
    "*<center>Jakub Gros, Sebastian Kucharzyk</center>*\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cel i wstępna analiza problemu\n",
    "<p style='text-align: justify;'>\n",
    "Jako cel obraliśmy sobie stworzenie maszynowego odpowiednika człowieka grającego w grę \n",
    "komputerową(tzw. agenta), który ucząc się na własnych błędach, będzie osiągał w niej coraz to lepsze \n",
    "rezultaty. Do tego celu nadaje się m.in. paradygmat procesu uczenia nazywany uczeniem ze wzmocnieniem,\n",
    "polegający na umieszczeniu uczącego się agenta w nieznanym mu dotąd środowisku i pozwoleniu na \n",
    "wykonywanie dowolnych interakcji z otoczeniem. Agent w trakcie eksploracji doświadcza wielu różnych \n",
    "sytuacji zwanych stanami. Znajdując się w danym stanie wybiera, którą z obecnie możliwych akcji \n",
    "wykona, za co  finalnie otrzymuje pewną nagrodę lub karę. Wszystkie informacje na temat ruchów i \n",
    "nagród przechowuje w pamięci, aby nastepnie na ich podstawie móc oceniać, jaka akcja da mu \n",
    "największą nagrodę.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Pozornie wydawałoby się, że rozwiązanie to nadaje się niemal idealnie do sytuacji \n",
    "wymagających, aby agent sam zebrał dane, na których następnie będzie się uczył, ale niestety posiada ono \n",
    "jedną znaczącą wadę - wraz z upływem czasu ilość danych w pamięci agenta rośnie, co z kolei powoduje \n",
    "spowolnienie podejmowania decyzji, a finalnie może nawet doprowadzić do zapełnienia pamięci \n",
    "fizycznej urządzenia.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Aby uniknąć problemów związanych z nadmiarem danych, zdecydowaliśmny się na wykorzystanie innego paradygmatu, \n",
    "który pozwala zgeneralizować zbiór danych zamiast nieskończenie go poszerzać i przeszukiwać. Dokładniej \n",
    "rzecz ujmując, wybór padł na zastosowanie głębokiego Q-learningu, czyli połączenia uczenia \n",
    "ze wzmocnieniem z uczeniem głębokim. Polega on na zastąpieniu nieograniczonej pamięci agenta siecią neuronową \n",
    "wspomaganą relatywnie małym buforem pamiętającym N ostatnich wyborów. Ponadto po każdej akcji podjętej przez agenta\n",
    "z buforu wybierany jest mały fragment danych, na których następnie sieć jest dotrenowywana.\n",
    "Wykonuje się to za pomocą Q-wartości, czyli pewnej funkcji  <b>Q(S, A)</b> szacującej jak bardzo opłaca się \n",
    "podjąć akcję A będąc w stanie S. Cały powyższy proces prowadzi do stworzenia modelu Deep Q-Network(DQN).\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Tutaj zakres projektu mógłby się zakończyć, ale nie chcieliśmy się ograniczać jedynie do głębokiego Q-learningu, \n",
    "więc postanowiliśmy znaleźć zastosowanie dla innego zagadnienia z zakresu stucznej inteligencji - algorytmów \n",
    "ewolucyjnych. Patrząc na sposób działania modelu DQN można zauważyć trzy obszary, które potencjalnie nadają się \n",
    "do ulepszenia ewolucyjnego:\n",
    "</p>\n",
    "\n",
    "1. Kształt sieci neuronowej\n",
    "2. Wagi połączeń międzyneuronowych\n",
    "3. Dane przechowywane w pamięci agentów\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Po dokładnym przeanalizowaniu powyższych możliwości zdecydowaliśmy się odrzucić opcje numer 1 oraz 3. \n",
    "O wyeliminowaniu manipulacji kształtem sieci neuronowej zadecydował fakt, że wolimy skupić się na\n",
    "stworzeniu jednego wariantu sieci i jego dogłębnej analizie niż na pobieżnym tworzeniu wielu modeli tylko \n",
    "po to, żeby różniły się budową. Natomiast jeśli chodzi o opcję trzecią, to jej odrzucenie wynika z \n",
    "natury DQN, której agent przechowuje w pamięci jedynie mały  fragment z całego zbioru danych, \n",
    "na którym był uczony. W takim wypadku używanie go jako podstawy do dalszego ulepszania najprawdopodobniej nie \n",
    "prowadziłoby do uzyskania wymiernych korzyści. W ten sposób stwierdziliśmy, że najciekawszym i (oby) \n",
    "najefektywniejszym podejściem będzie zastosowanie algorytmu genetycznego operującego na wagach połączeń \n",
    "międzyneuronowych.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Etapy działania\n",
    "Podsumowując przeprowadzoną powyżej analizę, zdecydowaliśmy się podzielić realizację projektu na następujące \n",
    "fazy:\n",
    "1. Implementacja i gromadzenie rezultatów naiwnego agenta, który podejmuje decyzje w sposób losowy. Posłuży on \n",
    "jako wyznacznik do porównywania innych modeli.\n",
    "2. Implementacja i gromadzenie rezultatów agenta wykorzystującego prostą sieć neuronową.\n",
    "3. Modyfikacja implementacji agenta oraz parametrów sieci neuronowej w celu poprawy osiąganych rezultatów.\n",
    "4. Wytrenowanie początkowej populacji agentów i zastosowanie algorytmu genetycznego do stworzenia osobnika z \n",
    "jak najlepiej dostosowanymi wagami międzyneuronowymi.\n",
    "5. Porównanie osiągniętych rezultatów i wyciągnięcie wniosków z doświadczenia.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sposób przeprowadzania testów\n",
    "<p style='text-align: justify;'>\n",
    "Aby ocenić jakość modelu, należy najpierw go przetestować pod kątem stabilności oraz wiarygodności. Model \n",
    "staje  się stabilny w momencie, gdy wyniki osiągane w kolejnych grach przyjmują zbliżone do siebie wartości. \n",
    "Aby osiągnąć stan uznawany za  stabilny, wymagane jest trenowanie modelu na odpowiedniej (nie za małej) liczbie gier. \n",
    "Z kolei jeśli chodzi o wiarygodność, to osiągane przez dany model wyniki uznajemy za wiarygodne, kiedy nie są wypaczone \n",
    "przez losowe warunki początkowe. Aby określić wiarygodność wyników, należy wielokrotnie wytrenować model i określić \n",
    "ich tendencję, na przykład poprzez wyciągnięcie z nich średniej.\n",
    "\n",
    "\n",
    "Natomiast wiarygodność danego modelu testujemy za pomocą wielokrotnego trenowania go od podstaw, ponieważ wyniki po \n",
    "tylko jednej próbie mogą być wypaczone ze względu na losowe warunki początkowe. Na przykład model z bardzo dobrymi predyspozycjami \n",
    "do danego zadania może trafić na złe warunki początkowe w trakcie trenowania, przez co wyniki przez niego osiągane \n",
    "będą co najwyżej mierne. Natomiast inny model o słabych predyspozycjach do danego zadania może trafić na niemalże perfekcyjne \n",
    "warunki początkowe, przez co jego rezultaty będą złudnie dobre, jednak przy próbie ponownego jego wytrenowania rezultaty \n",
    "te będą zgodne z jego predyspozycjami. Przez to większa liczba trenowań od podstaw daje równe szanse wariantom modeli na zaprezentowanie swoich\n",
    "możliwości i wiarygodną ich ocenę.\n",
    "\n",
    "\n",
    "Większa ilość treningów daje agentom równe szanse na podobne wyniki\n",
    "\n",
    "Wiarygodny sposób testowania i porównywania wyników osiąganych przez poszczególne wersje agentów \n",
    "\n",
    "wypracowaliśmy dopiero podczas zaawansowanych prac nad projektem (wynika to z faktu, że musieliśmy nabyć swego \n",
    "rodzaju obycie w środowisku Cart Pole). Wtedy też zauważyliśmy, że zdecydowana większość agentów zaczyna osiągać \n",
    "stabilne rezultaty po około 200 rozegranych grach. \n",
    "\n",
    "Ponadto każdy z agentów na początku uczy się interakcji ze \n",
    "środowiskiem w sposób losowy, więc część agentów może mieć po prostu szczęście, co również należy wziąć pod uwagę przy \n",
    "porównywaniu poszczególnych wariantów modeli ze sobą. Tak więc zdecydowaliśmy, że wariant będziemy trenować \n",
    "dokładnie trzy razy, za każdym razem przeznaczając około 100 gier na naukę i eksplorację, a następne 100 gier \n",
    "do oceny, czyli z wyników w nich osiągniętych zostanie wyciągnięta średnia, aby na samym końcu wyciągnąć średnią \n",
    "ze średnich ze wszystkich trzech trenowań danego agenta.\n",
    "</p>\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ograniczenia sprzętowe\n",
    "<p style='text-align: justify;'>\n",
    "W momencie gdy zaimplementowaliśmy prostego i stosunkowo uniwersalnego agenta, który mógł być \n",
    "trenowany z wykorzystaniem większości środowisk dostępnych poprzez interfejs AI Gym, okazało się, \n",
    "że nasz sprzęt nie jest wystarczająco wydajny, aby zrealizować ten projekt w akceptowalnych ramach \n",
    "czasowych. Mówiąc konkretniej, trenowanie naszego modelu do stopnia, w którym jego wyniki są \n",
    "powtarzalne i można zacząć wyciągać z nich wnioski, w najprostszym środowisku(CartPole) zajmuje \n",
    "około 10 minut, natomiast w drugim co do złożoności LunarLander czas ten wzrasta aż do 45 minut. \n",
    "Po oszacowaniu, że do zakończenia projektu będziemy musieli trenować model w różnych konfiguracjach \n",
    "blisko 100 razy, liczba 75 godzin spędzona jednynie na trenowaniu LunarLandera byłaby zbyt duża.\n",
    "</p>\n",
    "\n",
    "## Zastosowane technologie\n",
    "<p style='text-align: justify;'>\n",
    "Kod projektu napisaliśmy w języku Python w wersji 3.6, z którego znakomita większość wykonuje się \n",
    "na procesorze. Karta graficzna jest odpowiedzialna za przeprowadzanie operacji czerpiących wymierne \n",
    "korzyści z wielowątkowości, a więc za trenowanie sieci neuronowej. Sama sieć oraz wszelkie operacje \n",
    "z nią związane pochodzą z wysokopoziomowego API Kerasa bazującego na bibliotece TensorFlow. Jeśli \n",
    "zaś chodzi o środowisko, w którym odbywa się trenowanie, to jest to CartPole z biblioteki AI Gym.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Celem projektu jest stworzenie i wytrenowanie agenta DQN w taki sposób, aby osiągał jak najlepsze rezultaty w grze\n",
    "typu Cart Pole.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model losowy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "a = random.randint()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NUM_OF_EPISODES = 200\n",
    "FRAMES_PER_EPISODE = 1000\n",
    "BATCH_SIZE = 32\n",
    "GAME_ID = 'CartPole-v1'\n",
    "CPU_ID = '/device:CPU:0'\n",
    "print(0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Header"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Header"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Header"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Header"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}