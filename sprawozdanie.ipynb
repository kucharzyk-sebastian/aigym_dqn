{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# <center>Agent DQN doszkalany ewolucyjnie</center>\n",
    "<center><img src=\"videos\\naive_0.gif\" width=\"460\" height=\"280\" /></center>\n",
    "\n",
    "*<center>Wykonali</center>*\n",
    "*<center>Jakub Gros, Sebastian Kucharzyk</center>*\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <center>Część teoretyczna</center>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cel i wstępna analiza problemu\n",
    "<p style='text-align: justify;'>\n",
    "Jako cel obraliśmy sobie stworzenie maszynowego odpowiednika człowieka grającego w grę \n",
    "komputerową(tzw. agenta), który ucząc się na własnych błędach, będzie osiągał w niej coraz to lepsze \n",
    "rezultaty. Do tego celu nadaje się m.in. paradygmat procesu uczenia nazywany uczeniem ze wzmocnieniem,\n",
    "polegający na umieszczeniu uczącego się agenta w nieznanym mu dotąd środowisku i pozwoleniu na \n",
    "wykonywanie dowolnych interakcji z otoczeniem. Agent w trakcie eksploracji doświadcza wielu różnych \n",
    "sytuacji zwanych stanami. Znajdując się w danym stanie wybiera, którą z obecnie możliwych akcji \n",
    "wykona, za co  finalnie otrzymuje pewną nagrodę lub karę. Wszystkie informacje na temat ruchów i \n",
    "nagród przechowuje w pamięci, aby nastepnie na ich podstawie móc oceniać, jaka akcja da mu \n",
    "największą nagrodę.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Pozornie wydawałoby się, że rozwiązanie to nadaje się niemal idealnie do sytuacji \n",
    "wymagających, aby agent sam zebrał dane, na których następnie będzie się uczył, ale niestety posiada ono \n",
    "jedną znaczącą wadę - wraz z upływem czasu ilość danych w pamięci agenta rośnie, co z kolei powoduje \n",
    "spowolnienie podejmowania decyzji, a finalnie może nawet doprowadzić do zapełnienia pamięci \n",
    "fizycznej urządzenia.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Aby uniknąć problemów związanych z nadmiarem danych, zdecydowaliśmny się na wykorzystanie innego paradygmatu, \n",
    "który pozwala zgeneralizować zbiór danych zamiast nieskończenie go poszerzać i przeszukiwać. Dokładniej \n",
    "rzecz ujmując, wybór padł na zastosowanie głębokiego Q-learningu, czyli połączenia uczenia \n",
    "ze wzmocnieniem z uczeniem głębokim. Polega on na zastąpieniu nieograniczonej pamięci agenta siecią neuronową \n",
    "wspomaganą relatywnie małym buforem pamiętającym N ostatnich wyborów. Ponadto po każdej akcji podjętej przez agenta\n",
    "z buforu wybierany jest mały fragment danych, na których następnie sieć jest dotrenowywana.\n",
    "Wykonuje się to za pomocą Q-wartości, czyli pewnej funkcji  <b>Q(S, A)</b> szacującej jak bardzo opłaca się \n",
    "podjąć akcję A będąc w stanie S. Cały powyższy proces prowadzi do stworzenia modelu Deep Q-Network(DQN).\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Tutaj zakres projektu mógłby się zakończyć, ale nie chcieliśmy się ograniczać jedynie do głębokiego Q-learningu, \n",
    "więc postanowiliśmy znaleźć zastosowanie dla innego zagadnienia z zakresu stucznej inteligencji - algorytmów \n",
    "ewolucyjnych. Patrząc na sposób działania modelu DQN można zauważyć trzy obszary, które potencjalnie nadają się \n",
    "do ulepszenia ewolucyjnego:\n",
    "</p>\n",
    "\n",
    "1. Kształt sieci neuronowej\n",
    "2. Wagi połączeń międzyneuronowych\n",
    "3. Dane przechowywane w pamięci agentów\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Po dokładnym przeanalizowaniu powyższych możliwości zdecydowaliśmy się odrzucić opcje numer 1 oraz 3. \n",
    "O wyeliminowaniu manipulacji kształtem sieci neuronowej zadecydował fakt, że wolimy skupić się na\n",
    "stworzeniu jednego wariantu sieci i jego dogłębnej analizie niż na pobieżnym tworzeniu wielu modeli tylko \n",
    "po to, żeby różniły się budową. Natomiast jeśli chodzi o opcję trzecią, to jej odrzucenie wynika z \n",
    "natury DQN, której agent przechowuje w pamięci jedynie mały  fragment z całego zbioru danych, \n",
    "na którym był uczony. W takim wypadku używanie go jako podstawy do dalszego ulepszania najprawdopodobniej nie \n",
    "prowadziłoby do uzyskania wymiernych korzyści. W ten sposób stwierdziliśmy, że najciekawszym i (oby) \n",
    "najefektywniejszym podejściem będzie zastosowanie algorytmu genetycznego operującego na wagach połączeń \n",
    "międzyneuronowych.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Etapy działania\n",
    "Podsumowując przeprowadzoną powyżej analizę, zdecydowaliśmy się podzielić realizację projektu na następujące \n",
    "fazy:\n",
    "1. Implementacja i gromadzenie rezultatów naiwnego agenta, który podejmuje decyzje w sposób losowy. Posłuży on \n",
    "jako wyznacznik do porównywania innych modeli.\n",
    "2. Implementacja i gromadzenie rezultatów agenta wykorzystującego prostą sieć neuronową.\n",
    "3. Modyfikacja implementacji agenta oraz parametrów sieci neuronowej w celu poprawy osiąganych rezultatów.\n",
    "4. Wytrenowanie początkowej populacji agentów i zastosowanie algorytmu genetycznego do stworzenia osobnika z \n",
    "jak najlepiej dostosowanymi wagami międzyneuronowymi.\n",
    "5. Porównanie osiągniętych rezultatów i wyciągnięcie wniosków z doświadczenia.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ocena jakości modelu\n",
    "<p style='text-align: justify;'>\n",
    "Aby rzetelnie ocenić jakość modelu, należy najpierw upewnić się, że wyniki przez niego osiągane są stabilne oraz \n",
    "wiarygodne. Mianem stabilnych określamy je wtedy, gdy w trakcie rozgrywania kolejnych gier przyjmują one zbliżone \n",
    "do siebie wartości. Aby osiągnąć taki rezultat, wymagane jest trenowanie modelu na odpowiedniej \n",
    "(nie za małej) liczbie gier. Metodą prób i błędów określiliśmy, że dla różnych modeli w naszym środowisku treningowym\n",
    "liczba ta oscyluje przeważnie w granicach 200.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Z kolei jeśli chodzi o wiarygodność, to uzyskiwane przez dany model rezultaty uznajemy za wiarygodne wtedy, gdy \n",
    "nie są one wypaczone przez losowe warunki początkowe. Istnieją dwa różne podejścia pomagające w zapewnieniu \n",
    "wiarygodności wyników:\n",
    "</p>\n",
    "\n",
    "1. Trenowanie każdego modeli w identycznych warunków początkowych\n",
    "2. Zastosowanie pewnej funkcji oceny, np. wyciągnięcia średniej z kilku różnych prób wytrenowania tego samego modelu.\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Ze względu na to, że ideą głębokiego Q-learningu jest eksploracja środowiska w sposób losowy, nasz wybór musiał \n",
    "paść na opcję numer 2.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Zważając na powyższe rozważania zdecydowaliśmy, że każdy z naszych modeli będzie trenowany dokładnie cztery \n",
    "razy po dwieście gier. Z rezultatów osiąganych we wszystkich czterech podejściach wyciągniemy średnią, a \n",
    "następnie porównamy ją z innymi modelami.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zastosowane technologie\n",
    "<p style='text-align: justify;'>\n",
    "Kod projektu napisaliśmy w języku Python w wersji 3.6. Sieć neuronowa oraz wszelkie struktury i metody\n",
    "z nią  związane pochodzą z wysokopoziomowego API Kerasa bazującego na bibliotece TensorFlow. Jeśli zaś \n",
    "chodzi o samo środowisko do trenowania agenta, wykorzystaliśmy zestaw narzędzi AI Gym, a dokładniej\n",
    "grę Cart Pole. Ponadto, aby maksymalnie wykorzystać możliwości sprzętowe zdecydowaliśmy, że \n",
    "części kodu wymagające wykonywania sekwencyjnego będą uruchamiane z użyciem procesora, a fragmenty \n",
    "możliwe do zrównoleglenia obsłuży karta graficzna. \n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dygresja na temat podjętych decyzji\n",
    "<p style='text-align: justify;'>\n",
    "Pierwszą wersję modelu stworzyliśmy z myślą o uniwersalności. Zasadniczo była ona zdolna do współpracy z \n",
    "większością środowisk dostępnych poprzez interfejs AI Gym. Dzięki temu zabiegowi byliśmy w stanie przeprowadzić \n",
    "serię prób pozwalających na wyłonienie najlepszego kandydata do dalszego ulepszania modelu. Testy zaczęliśmy od \n",
    "najbardziej złożonych środowisk (BipedalWalker-v2 - kombinacje 8 różnych możliwych do podjęcia akcji) poprzez te średnio \n",
    "skomplikowane (LunarLander-v2 - możliwe do wykonania 4 akcje) aż po najłatwiejsze (CartPole-v1 - tylko 2 możliwe akcje). \n",
    "Niestety okazało się, że nasz sprzęt nie jest w stanie wytrenować sieci neuronowej dla najtrudniejszego \n",
    "środowiska w racjonalnych ramach czasowych, więc postanowiliśmy skupić się na grze o średniej truności - Lunar Lander.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Mówiąc konkretniej, samo trenowanie jednego modelu do stopnia, w którym jego wyniki są powtarzalne i pozwalają na \n",
    "wyciąganie sensownych wniosków, zajmuje około 10 minut w najprostszym środowisku (CartPole), w drugim \n",
    "co do złożoności LunarLander czas ten jest około 5 krotnie większy, natomiast w BipedalWalker jest aż 20 krotnie większy. \n",
    "Idąc dalej, po oszacowaniu, że do zakończenia projektu będziemy musieli trenować model w różnych konfiguracjach blisko \n",
    "100 razy, najtrudniejszym środowiskiem na jakie mogliśmy sobie pozwolić był Lunar Lander.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Opis środowiska LunarLander-v2\n",
    "<p style='text-align: justify;'>\n",
    "Lądowisko ma zawsze współrzędne (0,0). Współrzędne to dwie pierwsze liczby w wektorze stanu. Nagroda za \n",
    "przebycie drogi od góry ekranu aż do lądowiska i do prędkości 0 to około 100-140 punktów. \n",
    "Gra kończy się w momencie, gdy lądownik rozbija się lub ląduje, otrzymując dodatkowe -100 lub +100 punktów. Każda noga,\n",
    "która styka się z ziemią to dodatkowe +10 punktów. Uruchamianie głównego silinika to -0.3 punkta na klatkę. Skuteczne \n",
    "lądowanie to 200 punktów. Lądowanie poza lądowiskiem jest możliwe. Paliwo jest nieskończone, więc agent może najpierw \n",
    "nauczyć się latać, a potem wylądować za pierwszym razem. Możliwe do wykonania 4 dyskretne akcje: nie rób nic, uruchom \n",
    "lewy silnik, uruchom główny silnik, uruchom prawy silnik przyjmują kolejno wartości 0, 1, 2 oraz 3.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <center>Część praktyczna</center>\n",
    "## Preludium\n",
    "<p style='text-align: justify;'>\n",
    "W poniższych podrozdziałach zawarliśmy przegląd wszystkich wykonanych przez nas modeli. Przedstawione są one w \n",
    "sposób chronologiczny względem ich powstania. Prezentacji podlega zarówno kod źródłowy jak i osiągane wyniki. \n",
    "Ponadto pozwoliliśmy sobie pominąć komentowanie oczywistych części kodu, aby nie zaciemniać tego, \n",
    "co najważniejsze, czyli rezultatów i wniosków z tworzenia kolejnych modeli. Natomiast jeśli chodzi o fragmenty,\n",
    "które uznaliśmy za ważne, to są one opisane bezpośrednio w kodzie lub w kolejnym akapicie pod nim.\n",
    "</p>\n",
    "\n",
    "***\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <b>Naiwny agent losowy</b>\n",
    "Pierwszy zaimplementowany przez nas agent podejmuje decyzje o tym, jaką akcę wykonać, w sposób losowy. Stanowi \n",
    "swego rodzaju obiekt porównawczy dla późniejszych modeli. Jego implementacja wygląda następująco:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "import gym     # Pełni rolę interfejsu dla środowiska LunarLander"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NUM_OF_AGENTS = 4          # liczba agentów którzy będą wytrenowani od podstaw.\n",
    "NUM_OF_EPISODES = 100      # liczba gier rozegranych przez  jednego agenta.\n",
    "FRAMES_PER_EPISODE = 1000  # liczba klatek równa liczbie akcji do podjęcia przez agenta w każdej grze\n",
    "GAME_ID = \"LunarLander-v2\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "game = gym.make(GAME_ID)             # Tworzymy instancję określonej wcześniej gry\n",
    "num_of_actions = game.action_space.n # wyłuskujemy ilość akcji, które agent ma do wyboru\n",
    "is_done = False\n",
    "avgs = []\n",
    "for model in range(NUM_OF_AGENTS): \n",
    "    scores = []\n",
    "    for episode in range(NUM_OF_EPISODES):\n",
    "        current_state = game.reset()\n",
    "        total_score = 0\n",
    "        for frame in range(FRAMES_PER_EPISODE):\n",
    "            # game.render()\n",
    "            action = random.randrange(num_of_actions) # wybór losowej akcji z zakresu możliwych\n",
    "            new_state, gained_reward, is_done, info = game.step(action) \n",
    "            total_score += gained_reward\n",
    "            if is_done:\n",
    "                break\n",
    "        scores.append(total_score)             # zapamiętujemy wynik po zakończeniu gry\n",
    "    avgs.append(sum(scores)/len(scores)) # zapamiętujemy średni wynik osiągniety przez agenta\n",
    "game.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Do wytrenowania agenta potrzebujemy dwóch metod z interfejsu <b>gym</b> operujących na grze:\n",
    "1. <b>reset()</b> - inicjalizuje nową grę oraz zwraca stan początkowy agenta\n",
    "2. <b>step(action)</b> - wykonuje akcję podaną jako argument, zwraca kolejno: nowy stan, zdobytą nagrodę, flagę wskazującą \n",
    "czy gra się zakończyła oraz obiekt zawierający informacje dla programisty (debug info)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 has avarage: -181.27832294252718\n",
      "Model 1 has avarage: -191.51285271308996\n",
      "Model 2 has avarage: -182.1684255640586\n",
      "Model 3 has avarage: -194.19551136274666\n",
      "Overall avg: -187.2887781456056\n"
     ]
    }
   ],
   "source": [
    "for i, avg in enumerate(avgs): # Wyświetlamy wyniki poszczególnych agentów\n",
    "    print(\"Model {} has avarage: {}\".format(i, avg))\n",
    "print(\"Overall avg: {}\".format(sum(avgs)/len(avgs)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Wnioski\n",
    "<p style='text-align: justify;'>\n",
    "Działania agenta były losowe, więc zgodnie z przewidywaniami osiągane przez niego wyniki były słabe. Poniżej można \n",
    "zobaczyć krótkie nagranie przedstawiające sposób jego działania:\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center><img src=\"videos\\naive_0.gif\" width=\"460\" height=\"280\" /></center>\n",
    "\n",
    "***\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agent DQN v1\n",
    "<p style='text-align: justify;'>\n",
    "Następnym krokiem było usprawnienie naiwnego agenta poprzez dodanie prostej sieci neuronowej oraz metod\n",
    "odpowiedzialnych za jej trenowanie. \n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf # używamy go do określenia, który fragment kodu wykonuje się na GPU"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SimpleDqnNpc:\n",
    "    \"Klasa implementująca agenta DQN opartego o prostą sieć neuronową\"\n",
    "\n",
    "    def __init__(self, num_of_inputs, num_of_outputs):\n",
    "        \"\"\"\n",
    "        num_of_inputs - długość wektora będącego wejściem dla sieci neuronowej\n",
    "        num_of_outputs - ilość wyjść z sieci neuronowej\n",
    "        \"\"\"\n",
    "\n",
    "        self._num_of_inputs = num_of_inputs\n",
    "        self._num_of_outputs = num_of_outputs\n",
    "        self._exploration_rate = 1.0           # Prawdopodobieństwo podjęcia losowej akcji\n",
    "        self._exploration_rate_min = 0.1\n",
    "        self._exploration_rate_decay = 0.997\n",
    "        self.memory = deque(maxlen=1024)\n",
    "        self._init_model()\n",
    "\n",
    "    def _init_model(self):\n",
    "        \"\"\"\n",
    "        Inicjalizuje model sieci neuronowej.\n",
    "        \"\"\"\n",
    "\n",
    "        self._model = Sequential()\n",
    "        self._model.add(Dense(self._num_of_inputs, input_dim=self._num_of_inputs, activation='linear'))\n",
    "        self._model.add(Dense(self._num_of_outputs, activation='linear'))\n",
    "        self._model.compile(optimizer=SGD(), loss='mean_squared_error')\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Przewiduje i zwraca akcję, którą należy wykonać\"\"\"\n",
    "\n",
    "        if np.random.rand() <= self._exploration_rate:\n",
    "            return random.randrange(self._num_of_outputs)\n",
    "        act_values = self._model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def retain(self, current_state, taken_action, gained_reward, future_state, is_lost):\n",
    "        \"\"\"Zapisuje dany przypadek w pamięci agenta\"\"\"\n",
    "\n",
    "        self.memory.append((current_state, taken_action, gained_reward, future_state, is_lost))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"\n",
    "        Doszkala sieć neuronową na losowym fragmencie z jego pamięci\n",
    "        batch-size - rozmiar fragmentu pamięci\n",
    "        \"\"\"\n",
    "\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        for current_state, taken_action, gained_reward, future_state, is_done in batch:\n",
    "            next_act_best_profit = gained_reward\n",
    "            if not is_done:\n",
    "                future_act_profits = self._model.predict(future_state)\n",
    "                next_act_best_profit = np.amax(future_act_profits[0])\n",
    "            current_act_profits = self._model.predict(current_state)\n",
    "            current_act_profits[0][taken_action] = next_act_best_profit\n",
    "            with tf.device('/device:GPU:0'):\n",
    "                self._model.fit(x=current_state, y=current_act_profits, epochs=1, verbose=0)\n",
    "        if self._exploration_rate > self._exploration_rate_min:\n",
    "            self._exploration_rate *= self._exploration_rate_decay\n",
    "        else:\n",
    "            self._exploration_rate = 0.0\n",
    "\n",
    "    def load(self, model_path):\n",
    "        \"\"\"Wczytuje model z pamięci\"\"\"\n",
    "\n",
    "        self._model.load_weights(model_path)\n",
    "\n",
    "    def save(self, model_path):\n",
    "        \"\"\"Zapisuje modele do pamięci\"\"\"\n",
    "\n",
    "        self._model.save_weights(model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> <p style='text-align: justify;'>\n",
    "Sieć neuronowa zaimplementowana w metodzie <b>_init_model()</b> została stworzona jako proste odzwierciedlenie posiadanych\n",
    "przez nas danych, czyli warstwa wejściowa ma ilość neuronów równą rozmiarowi danych wejściowych, a wyjściowa równą \n",
    "ilości możliwych do podjęcia akcji. Parametry takie jak funkcja aktywacji czy straty na razie wybraliśmy losowo.\n",
    "<br/>\n",
    "Implementacja metody <b>replay(batch_size)</b> jest jedną z najprostszych powszechnie stosowanych w modelach DQN. \n",
    "Wybiera małą próbkę danych (aby uniknąć znacznego spowolnienia całego procesu), a następnie dotrenowuje na niej model poprzez\n",
    "próbę przewidzenia najlepszej decyzji dla następnego stanu i skojarzenia jej z obecnym stanem. Można sobie to\n",
    "wyobrażać jako coraz dalsze i dalsze spoglądanie w przyszłość. Dzięki zastosowaniu podwójnie zakończonej kolejki\n",
    "(deque) jako pamięci, zyskujemy duże prawdopodobieństwo na kilkukrotne dotrenowanie modelu na danym przypadku, a jednocześnie\n",
    "uniknięcie przetrenowania. Dodatkowo po każdym wywołaniu tej metody obniżamy <b>współczynnik eksploracji</b> agenta, \n",
    "czyli zmniejszamy szansę na wykonanie losowego ruchu.\n",
    "</p>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NUM_OF_AGENTS = 4\n",
    "NUM_OF_EPISODES = 200\n",
    "FRAMES_PER_EPISODE = 1000\n",
    "BATCH_SIZE = 16              # Rozmiar próbki danych do dotrenowywania modelu\n",
    "GAME_ID = \"LunarLander-v2\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "game = gym.make(GAME_ID)\n",
    "num_of_actions = game.action_space.n\n",
    "observation_size = game.observation_space.shape[0]\n",
    "npc = SimpleDqnNpc(observation_size, num_of_actions)\n",
    "is_done = False\n",
    "avgs = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for model in range(NUM_OF_AGENTS):\n",
    "    scores = []\n",
    "    for episode in range(NUM_OF_EPISODES):\n",
    "        total_score = 0\n",
    "        current_state = np.reshape(game.reset(), [1, observation_size])\n",
    "        for frame in range(FRAMES_PER_EPISODE):\n",
    "            # game.render()\n",
    "            action = npc.act(current_state)\n",
    "            new_state, gained_reward, is_done, info = game.step(action)\n",
    "            new_state = np.reshape(new_state, [1, observation_size])\n",
    "            npc.retain(current_state, action, gained_reward, new_state, is_done)\n",
    "            total_score += gained_reward\n",
    "            current_state = new_state\n",
    "            if len(npc.memory) > BATCH_SIZE:\n",
    "                npc.replay(BATCH_SIZE)\n",
    "            if is_done:\n",
    "                break\n",
    "        scores.append(total_score)\n",
    "    npc.save(\"simple_dqn_\" + str(model) + \".h5\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "> <p style='text-align: justify;'>\n",
    "Zmiana kształtu zmiennych przechowujących stany wynika z implementacji kerasa. Żeby dane zostały poprawnie \n",
    "zinterpretowane należy umieścić je w zagnieżdżonych listach.\n",
    "</p>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 has avarage: -136.84274049982846\n",
      "Model 1 has avarage: -142.73024592812233\n",
      "Model 2 has avarage: -137.12955212352132\n",
      "Model 3 has avarage: -137.8901023058812\n",
      "Overall avg: -138.64816021433834\n"
     ]
    }
   ],
   "source": [
    "for i, avg in enumerate(avgs):\n",
    "    print(\"Model {} has avarage: {}\".format(i, avg))\n",
    "print(\"Overall avg: {}\".format(sum(avgs) / len(avgs)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Wnioski\n",
    "<p style='text-align: justify;'>\n",
    "Liczbowe wyniki agenta z prostą siecią neuronową są nieco lepsze od wyników agenta naiwnego. Po analizie wydarzeń oraz \n",
    "zapisu wideo można zauważyć, że agent rzeczywiście początkowo skupia się na eksploracji otoczenia\n",
    "uruchamiając silniki w przypadkowej kolejności. Niemniej jednak wraz z upływem kolejnych prób i zmniejszania się \n",
    "współczynnika eksploracji, zaczyna podejmować najlepsze (według niego) decyzje, czyli po prostu opadać w dół. \n",
    "Poniekąd jest to racjonalna decyzja, bo właśnie na tym polega Lunar Lander, jednakże agent nie nauczył się zwracać \n",
    "uwagi na te mniej oczywiste czynniki wspomagające sukces, takie jak stabilizacja statku czy niezbaczanie z pionowej\n",
    "trajektorii lotu. Wygląda na to, że model bardzo szybko wpadał w minimum lokalne. Mogło się tak stać, ponieważ po pewnym czasie \n",
    "ustaliliśmy wartość współczynnika eksploracji na 0, więc agent nie miał szansy odkryć niczego nowego.\n",
    "</p>\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Ponadto na ten efekt złożył się też fakt, że kształt i parametry naszej sieci neuronowej nie są zbyt dobrze dobrane. \n",
    "Przede wszystkim liniowa funkcja aktywacji dla obu warstw to marnowanie potencjału wynikającego ze stosowania sieci neuronowej. \n",
    "Jeśli dana warstwa neuronów używa liniowej funkcji aktywacji do określenia pobudzenia poszczególnych neuronów, a \n",
    "następnie przekazuje je jako impuls do kolejnej warstwy pobudzanej liniowo, to na końcu nie otrzymujemy niczego innego \n",
    "niż po prostu pewną funkcję liniową określoną na podstawie wartości wejść pierwszej. To oznacza, że wszystkie znajdujące się\n",
    "obok siebie warstwy z liniową funkcją aktywacji mogą po prostu zostać zamienione w jedną warstwę. Innym problemem funkcji\n",
    "liniowej jest to, że w miarę dotrenowywania jej wartości na wyjściu mogą zmierzać do nieskończoności, aż w pewnym\n",
    "momencie przepełnić stos i być interpretowane jako <i>NaN</i>.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Drugim, poza funkcją straty, rzucającym się w oczy parametrem jest funkcja optymalizacji. Użyliśmy algorytmu \n",
    "stochastycznego spadku wzdłuż gradientu, który skupia się na określeniu tendencji optymalizowanej funkcji \n",
    "biorąc pod uwagę jedynie położenie, w którym znajduje się w danym momencie. Jeśli zastanowimy się dłużej \n",
    "nad tym, jaki efekt chcemy osiągnąć w modelu DQN, to dojdziemy do wniosku, że ważne jest dla nas skupianie \n",
    "się nie na jednym, a na kilku krokach naprzód. \n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Poza tym jeśli spojrzymy na metody wykorzystywane przez  DQN, to też nie są one bez wad. Na przykład wewnątrz\n",
    "metody replay w trakcie przewidywania wartości dla następnego stanu i wpisywania jej do obecnego, nie skalujemy \n",
    "jej w żaden sposób.  To oznacza, że bezgranicznie wierzymy naszemu modelowi, że skutecznie przewiduje ruch, który\n",
    "należy wykonać (chociaż w cale tak nie jest). Również w tej samej metodzie w trakcie trenowania na stanach, które\n",
    "nie są stanami kończącymi grę, nie bierzemy pod uwagę zdobytych przez nie nagród.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center><img src=\"videos\\v1_0.gif\" width=\"460\" height=\"280\" /></center>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agent DQN v2\n",
    "<p style='text-align: justify;'>\n",
    "Biorąc pod uwagę wnioski, które wyciągnęliśmy z poprzedniego model, zdecydowaliśmy się na wykonanie następujących\n",
    "czynności:\n",
    "</p>\n",
    "\n",
    "1. Dodanie współczynnika niepewności <b>discountrate</b> wobec przewidzianego wyniku oraz uwzględnienie otrzymanej \n",
    "nagrody w każym przypadku.\n",
    "2. Rezygnacja z zerowania wartości współczynnika uczenia po osiągnięciu progu 0.1.\n",
    "3. Zmiana funkcji aktywacji pierwszej warstwy na <b>relu</b> ze względu na jej charakterystykę - dla wartości mniejszych od \n",
    "zera zwraca 0, dzięki czemu istnieje duże prawdopodobieństwo, że część neuronów nie zostanie pobudzona, \n",
    "co sprawi, że sieć neuronowa będzie działała szybciej, a w naszym przypadku jest to bardzo duża zaleta. Niemniej jednak\n",
    "w połączeniu z funkcją liniową na wyjściu, wciąż mógłby pojawić się problem z wartościami na wyjściu zmierzającymi do nieskończoności, więc zdecydowaliśmy się \n",
    "dodać ukrytą warstwę z sigmoidalną funkcją aktywacji, która zwraca wartości od 0 do 1, a więc skutecznie przeskaluje wyniki.\n",
    "4. Zmiana funkcji optymalizacji na <b>ADAM</b>, która usprawnia scholastyczny algorytm gradientu o dodanie wartości pędu, co \n",
    "pozwala na unikanie minimów lokalnych. Działa to zupełnie jak w fizyce - jeśli kula toczy się w dół zbocza i na drodze \n",
    "napotka kamień, to mimo wszystko będzie w stanie się po nim przetoczyć dzięki nabranemu wcześniej pędowi, więc ominie \n",
    "swoje minimum lokalne. \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SimpleDqnNpcV2:\n",
    "    \"Klasa implementująca agenta DQN opartego o prostą sieć neuronową\"\n",
    "\n",
    "    def __init__(self, num_of_inputs, num_of_outputs):\n",
    "        \"\"\"\n",
    "        num_of_inputs - długość wektora będącego wejściem dla sieci neuronowej\n",
    "        num_of_outputs - ilość wyjść z sieci neuronowej\n",
    "        \"\"\"\n",
    "\n",
    "        self._num_of_inputs = num_of_inputs\n",
    "        self._num_of_outputs = num_of_outputs\n",
    "        self._exploration_rate = 1.0\n",
    "        self._exploration_rate_min = 0.1\n",
    "        self._exploration_rate_decay = 0.997\n",
    "        self._discout_rate = 0.95   # wskaźnik \"nieufności\" wobec predykcji\n",
    "        self.memory = deque(maxlen=1024)\n",
    "        self._init_model()\n",
    "\n",
    "    def _init_model(self):\n",
    "        \"\"\"\n",
    "        Inicjalizuje model sieci neuronowej.\n",
    "        Wybraliśmy (w naszym mniemaniu) najproszte parametry i kształt.\n",
    "        \"\"\"\n",
    "\n",
    "        self._model = Sequential()\n",
    "        self._model.add(Dense(self._num_of_inputs, input_dim=self._num_of_inputs, activation='relu'))\n",
    "        self._model.add(Dense(self._num_of_inputs * 2, activation='sigmoid')) \n",
    "        self._model.add(Dense(self._num_of_outputs, activation='linear'))\n",
    "        self._model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Przewiduje i zwraca akcję, którą należy wykonać\"\"\"\n",
    "\n",
    "        if np.random.rand() <= self._exploration_rate:\n",
    "            return random.randrange(self._num_of_outputs)\n",
    "        act_values = self._model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def retain(self, current_state, taken_action, gained_reward, next_state, is_done):\n",
    "        \"\"\"Zapisuje dyn przypadku w pamięci agenta\"\"\"\n",
    "\n",
    "        self.memory.append((current_state, taken_action, gained_reward, next_state, is_done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"\n",
    "        Doszkala sieć neuronową na losowym fragmencie z jego pamięci\n",
    "        batch-size - rozmiar fragmentu pamięci\n",
    "        \"\"\"\n",
    "\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        for current_state, taken_action, gained_reward, next_state, is_done in batch:\n",
    "            next_act_best_profit = gained_reward\n",
    "            if not is_done:\n",
    "                future_act_profits = self._model.predict(next_state)\n",
    "                next_act_best_profit = gained_reward + self._discout_rate * np.amax(future_act_profits[0])\n",
    "            current_act_profits = self._model.predict(current_state)\n",
    "            current_act_profits[0][taken_action] = gained_reward + self._discout_rate * next_act_best_profit\n",
    "            with tf.device('/device:GPU:0'):\n",
    "                self._model.fit(x=current_state, y=current_act_profits, epochs=1, verbose=0)\n",
    "        if self._exploration_rate > self._exploration_rate_min:\n",
    "            self._exploration_rate *= self._exploration_rate_decay\n",
    "\n",
    "    def load(self, model_path):\n",
    "        \"\"\"Wczytuje model z pamięci\"\"\"\n",
    "\n",
    "        self._model.load_weights(model_path)\n",
    "\n",
    "    def save(self, model_path):\n",
    "        \"\"\"Zapisuje modele do pamięci\"\"\"\n",
    "\n",
    "        self._model.save_weights(model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NUM_OF_AGENTS = 4\n",
    "NUM_OF_EPISODES = 200\n",
    "FRAMES_PER_EPISODE = 1000\n",
    "BATCH_SIZE = 16\n",
    "GAME_ID = \"LunarLander-v2\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "game = gym.make(GAME_ID)\n",
    "num_of_actions = game.action_space.n\n",
    "observation_size = game.observation_space.shape[0]\n",
    "npc = SimpleDqnNpcV2(observation_size, num_of_actions)\n",
    "is_done = False\n",
    "avgs = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for model in range(NUM_OF_AGENTS):\n",
    "    scores = []\n",
    "    for episode in range(NUM_OF_EPISODES):\n",
    "        score = 0\n",
    "        current_state = np.reshape(game.reset(), [1, observation_size])\n",
    "        for frame in range(FRAMES_PER_EPISODE):\n",
    "            # game.render()\n",
    "            action = npc.act(current_state)\n",
    "            new_state, gained_reward, is_done, info = game.step(action)\n",
    "            new_state = np.reshape(new_state, [1, observation_size])\n",
    "            npc.retain(current_state, action, gained_reward, new_state, is_done)\n",
    "            score += gained_reward\n",
    "            current_state = new_state\n",
    "            if len(npc.memory) > BATCH_SIZE:\n",
    "                npc.replay(BATCH_SIZE)\n",
    "            if is_done:\n",
    "                break\n",
    "        scores.append(score)\n",
    "    npc.save(\"simple_dqn_\" + str(model) + \".h5\")\n",
    "    avgs.append(sum(scores) / len(scores))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 has avarage: -168.23305719787398\n",
      "Model 1 has avarage: -145.7944561029682\n",
      "Model 2 has avarage: -173.22079611413694\n",
      "Model 3 has avarage: -158.50128312239501\n",
      "Overall avg: -161.43739813434353\n"
     ]
    }
   ],
   "source": [
    "for i, avg in enumerate(avgs):\n",
    "    print(\"Model {} has avarage: {}\".format(i, avg))\n",
    "print(\"Overall avg: {}\".format(sum(avgs) / len(avgs)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Wnioski\n",
    "<p style='text-align: justify;'>\n",
    "Niestety tym razem wyniki agenta w liczbach prezentują się nieco gorzej. Aczkolwiek duży progres można zauważyć po \n",
    "przejrzeniu nagrań wideo. Przez około 70 pierwszych gier agent wyraźnie próbuje utrzymać trajektorię lotu w taki \n",
    "sposób, aby kolizja z podłożem miała miejsce dokładnie na lądowisku:\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center><img src=\"videos\\v2_0.gif\" width=\"460\" height=\"280\" /></center>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Po rozegraniu około 130 gier agent zaczął wypracowywać technikę bujania się w lewo i w prawo, aby następnie bezpiecznie \n",
    "wylądować(z winikami około -30pkt!), co z resztą kilkukrotnie mu się udało (niestety nie na lądowisku):\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center><img src=\"videos\\v2_1.gif\" width=\"460\" height=\"280\" /></center>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Gdy agent osiągnął próg blisko 160 gier okazało się, dlaczego wynik tego modelu jest gorszy od poprzednika.\n",
    "Agent po prostu zaniechał wszelkich prób lądowania i zaczął jedynie utrzymywać się nieruchomo w \n",
    "powietrzu:\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center><img src=\"videos\\v2_2.gif\" width=\"460\" height=\"280\" /></center>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stwierdziliśmy, że może to być efekt przetrenowania i postanowiliśmy przyjrzeć się temu podczas tworzenia kolejnego \n",
    "modelu.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <b>Agent DQN v3</b>\n",
    "<p style='text-align: justify;'>\n",
    "Mając na względzie wnioski po poprzednim modelu postanowiliśmy wykonać następujące modyfikacje:\n",
    "</p>\n",
    "\n",
    "1. Zwiększenie pamięc agenta, żeby douczał się na danych z szerszego okresu. Dzięki temu unikniemy sytuacji, \n",
    "w których agent spędzając zbyt dużo czasu w prawie niezmienionym stanie, będzie wielokrotnie trenował się na bardzo \n",
    "podobnych do siebie danych.\n",
    "2. Poszerzenie dwóch pierwszych warstw sieci neuronowej, aby uzyskać dodatkowe kombinacje i połączenia tworzące \n",
    "więcej ścieżek do aktywacji.\n",
    "3. Dodanie do każdej warstwy funkcji Dropout, która w trakcie trenowania dezaktywuje część neuronów równą\n",
    "przyjętemu jako argument ułamkowi.\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Podjęliśmy również próbę zwiększenia rozmiaru próbki doszkalającej (BATCH_SIZE), ale niestety szybko wycofaliśmy się \n",
    " z tego pomysłu, bo po raz kolejny dały o sobie znać ograniczenia sprzętowe. \n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adam"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SimpleDqnNpcV3:\n",
    "    \"Klasa implementująca agenta DQN opartego o prostą sieć neuronową\"\n",
    "\n",
    "    def __init__(self, num_of_inputs, num_of_outputs):\n",
    "        \"\"\"\n",
    "        num_of_inputs - długość wektora będącego wejściem dla sieci neuronowej\n",
    "        num_of_outputs - ilość wyjść z sieci neuronowej\n",
    "        \"\"\"\n",
    "\n",
    "        self._num_of_inputs = num_of_inputs\n",
    "        self._num_of_outputs = num_of_outputs\n",
    "        self._exploration_rate = 1.0\n",
    "        self._exploration_rate_min = 0.1\n",
    "        self._exploration_rate_decay = 0.997\n",
    "        self._discout_rate = 0.95\n",
    "        self.memory = deque(maxlen=4096)\n",
    "        self._init_model()\n",
    "\n",
    "    def _init_model(self):\n",
    "        \"\"\"\n",
    "        Inicjalizuje model sieci neuronowej.\n",
    "        Wybraliśmy (w naszym mniemaniu) najproszte parametry i kształt.\n",
    "        \"\"\"\n",
    "\n",
    "        self._model = Sequential()\n",
    "        self._model.add(Dense(5 * self._num_of_inputs, input_dim=self._num_of_inputs, activation='relu'))\n",
    "        self._model.add(Dropout(0.15))\n",
    "        self._model.add(Dense(4 * self._num_of_inputs, activation='sigmoid'))\n",
    "        self._model.add(Dropout(0.15))\n",
    "        self._model.add(Dense(self._num_of_outputs, activation='linear'))\n",
    "        self._model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Przewiduje i zwraca akcję, którą należy wykonać\"\"\"\n",
    "\n",
    "        if np.random.rand() <= self._exploration_rate:\n",
    "            return random.randrange(self._num_of_outputs)\n",
    "        act_values = self._model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def retain(self, current_state, taken_action, gained_reward, next_state, is_done):\n",
    "        \"\"\"Zapisuje dyn przypadku w pamięci agenta\"\"\"\n",
    "\n",
    "        self.memory.append((current_state, taken_action, gained_reward, next_state, is_done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"\n",
    "        Doszkala sieć neuronową na losowym fragmencie z jego pamięci\n",
    "        batch-size - rozmiar fragmentu pamięci\n",
    "        \"\"\"\n",
    "\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        for current_state, taken_action, gained_reward, next_state, is_done in batch:\n",
    "            next_act_best_profit = gained_reward\n",
    "            if not is_done:\n",
    "                future_act_profits = self._model.predict(next_state)\n",
    "                next_act_best_profit = gained_reward + self._discout_rate * np.amax(future_act_profits[0])\n",
    "            current_act_profits = self._model.predict(current_state)\n",
    "            current_act_profits[0][taken_action] = gained_reward + self._discout_rate * next_act_best_profit\n",
    "            with tf.device('/device:GPU:0'):\n",
    "                self._model.fit(x=current_state, y=current_act_profits, epochs=1, verbose=0)\n",
    "        if self._exploration_rate > self._exploration_rate_min:\n",
    "            self._exploration_rate *= self._exploration_rate_decay\n",
    "\n",
    "    def load(self, model_path):\n",
    "        \"\"\"Wczytuje model z pamięci\"\"\"\n",
    "\n",
    "        self._model.load_weights(model_path)\n",
    "\n",
    "    def save(self, model_path):\n",
    "        \"\"\"Zapisuje modele do pamięci\"\"\"\n",
    "\n",
    "        self._model.save_weights(model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NUM_OF_AGENTS = 4\n",
    "NUM_OF_EPISODES = 200\n",
    "FRAMES_PER_EPISODE = 1000\n",
    "BATCH_SIZE = 16\n",
    "GAME_ID = \"LunarLander-v2\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_of_actions = game.action_space.n\n",
    "observation_size = game.observation_space.shape[0]\n",
    "npc = SimpleDqnNpcV3(observation_size, num_of_actions)\n",
    "is_done = False\n",
    "avgs = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for model in range(NUM_OF_AGENTS):\n",
    "    scores = []\n",
    "    for episode in range(NUM_OF_EPISODES):\n",
    "        score = 0\n",
    "        current_state = np.reshape(game.reset(), [1, observation_size])\n",
    "        for frame in range(FRAMES_PER_EPISODE):\n",
    "            game.render()\n",
    "            action = npc.act(current_state)\n",
    "            new_state, gained_reward, is_done, info = game.step(action)\n",
    "            new_state = np.reshape(new_state, [1, observation_size])\n",
    "            npc.retain(current_state, action, gained_reward, new_state, is_done)\n",
    "            score += gained_reward\n",
    "            current_state = new_state\n",
    "            if len(npc.memory) > BATCH_SIZE:\n",
    "                npc.replay(BATCH_SIZE)\n",
    "            if is_done:\n",
    "                break\n",
    "        scores.append(score)\n",
    "        npc.save(\"simple_dqn_\" + str(model) + \".h5\")\n",
    "    avgs.append(sum(scores) / len(scores))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 has avarage: -120.22066950112930\n",
      "Model 1 has avarage: -117.20091066801283\n",
      "Model 2 has avarage: -138.07912080607048\n",
      "Model 3 has avarage: -129.04182009469854\n",
      "Overall avg: --126.13563026747778\n"
     ]
    }
   ],
   "source": [
    "for i, avg in enumerate(avgs):\n",
    "    print(\"Model {} has avarage: {}\".format(i, avg))\n",
    "print(\"Overall avg: {}\".format(sum(avgs) / len(avgs)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Wnioski\n",
    "<p style='text-align: justify;'>\n",
    "Zgodnie z przewidywaniami agent tym razem nie został przetrenowany. Co więcej, zdobył najlepsze dotychczas rezultaty, \n",
    "a nagrane fragmenty rozgrywki tylko to potwierdzają. Poniżej można zobaczyć poszczególne kamienie milowe osiągnięte \n",
    "przez ten model:\n",
    "</p>\n",
    "\n",
    "1. Jak zwykle niemrawe początki:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center><img src=\"videos\\v3_0.gif\" width=\"460\" height=\"280\" /></center>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Po około 20 grach agent podjął pierwsze próby lądowania. Co ciekawe - odkrył, że za używanie głównego silnika \n",
    "czeka go kara, więc praktycznie go nie używał:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center><img src=\"videos\\v3_1.gif\" width=\"460\" height=\"280\" /></center>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Po rozegraniu około 80 gier agent był w stanie lądować, ale wciąż miał problemy z trafieniem w lądowisko:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center><img src=\"videos\\v3_2.gif\" width=\"460\" height=\"280\" /></center>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Po 120 grach poprawiła się celność agenta. Tym razem był w stanie wylądować, ale nie wiedział, że musi wyłączyć silniki,\n",
    "żeby zakończyć epizod, więc bezwładnie pchał się w bok:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center><img src=\"videos\\v3_3.gif\" width=\"460\" height=\"280\" /></center>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Po blisko 160 grach agent nareszcie odkrył, że aby wygrać, musi wyłączyć silnik i tym samym zacząć zdobywać\n",
    "dodatnią sumę punktów. W poniższej próbie był 5 punktów na plusie:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center><img src=\"videos\\v3_4.gif\" width=\"460\" height=\"280\" /></center>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Finalnie model nauczył się lądować i w ciągu ostatnich 20 gier wylądował aż trzykrotnie zdobywając kolejno\n",
    "40, 20 oraz 80 punktów na plusie. Poniżej prezentujemy to najlepsze lądowanie:\n",
    "<center><img src=\"videos\\v3_5.gif\" width=\"460\" height=\"280\" /></center>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Posłowie\n",
    "<p style='text-align: justify;'>\n",
    "Naszym planem na dalsze usprawnienie sieci neuronowej było przeanalizowanie wag wszystkich połączeń \n",
    "aby wykryć te, których wartości są bliskie zera. Jendakże zadanie to byłoby niezwykle \n",
    "czasochłonne, więc zdecydowaliśmy się zakończyć prace nad siecią na tym etapie, \n",
    "aby przeprowadzić eksperyment z algorytmem genetycznym.\n",
    "</p>\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}