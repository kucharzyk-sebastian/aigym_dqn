{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# <center>Agent DQN doszkalany ewolucyjnie</center>\n",
    "<center><img src=\"https://media.giphy.com/media/dyde6O8yn4oRh7R1Vk/source.gif\" width=\"460\" height=\"280\" /></center>\n",
    "\n",
    "*<center>Wykonali</center>*\n",
    "*<center>Jakub Gros, Sebastian Kucharzyk</center>*\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <center>Część teoretyczna</center>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cel i wstępna analiza problemu\n",
    "<p style='text-align: justify;'>\n",
    "Jako cel obraliśmy sobie stworzenie maszynowego odpowiednika człowieka grającego w grę \n",
    "komputerową(tzw. agenta), który ucząc się na własnych błędach, będzie osiągał w niej coraz to lepsze \n",
    "rezultaty. Do tego celu nadaje się m.in. paradygmat procesu uczenia nazywany uczeniem ze wzmocnieniem,\n",
    "polegający na umieszczeniu uczącego się agenta w nieznanym mu dotąd środowisku i pozwoleniu na \n",
    "wykonywanie dowolnych interakcji z otoczeniem. Agent w trakcie eksploracji doświadcza wielu różnych \n",
    "sytuacji zwanych stanami. Znajdując się w danym stanie wybiera, którą z obecnie możliwych akcji \n",
    "wykona, za co  finalnie otrzymuje pewną nagrodę lub karę. Wszystkie informacje na temat ruchów i \n",
    "nagród przechowuje w pamięci, aby nastepnie na ich podstawie móc oceniać, jaka akcja da mu \n",
    "największą nagrodę.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Pozornie wydawałoby się, że rozwiązanie to nadaje się niemal idealnie do sytuacji \n",
    "wymagających, aby agent sam zebrał dane, na których następnie będzie się uczył, ale niestety posiada ono \n",
    "jedną znaczącą wadę - wraz z upływem czasu ilość danych w pamięci agenta rośnie, co z kolei powoduje \n",
    "spowolnienie podejmowania decyzji, a finalnie może nawet doprowadzić do zapełnienia pamięci \n",
    "fizycznej urządzenia.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Aby uniknąć problemów związanych z nadmiarem danych, zdecydowaliśmny się na wykorzystanie innego paradygmatu, \n",
    "który pozwala zgeneralizować zbiór danych zamiast nieskończenie go poszerzać i przeszukiwać. Dokładniej \n",
    "rzecz ujmując, wybór padł na zastosowanie głębokiego Q-learningu, czyli połączenia uczenia \n",
    "ze wzmocnieniem z uczeniem głębokim. Polega on na zastąpieniu nieograniczonej pamięci agenta siecią neuronową \n",
    "wspomaganą relatywnie małym buforem pamiętającym N ostatnich wyborów. Ponadto po każdej akcji podjętej przez agenta\n",
    "z buforu wybierany jest mały fragment danych, na których następnie sieć jest dotrenowywana.\n",
    "Wykonuje się to za pomocą Q-wartości, czyli pewnej funkcji  <b>Q(S, A)</b> szacującej jak bardzo opłaca się \n",
    "podjąć akcję A będąc w stanie S. Cały powyższy proces prowadzi do stworzenia modelu Deep Q-Network(DQN).\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Tutaj zakres projektu mógłby się zakończyć, ale nie chcieliśmy się ograniczać jedynie do głębokiego Q-learningu, \n",
    "więc postanowiliśmy znaleźć zastosowanie dla innego zagadnienia z zakresu stucznej inteligencji - algorytmów \n",
    "ewolucyjnych. Patrząc na sposób działania modelu DQN można zauważyć trzy obszary, które potencjalnie nadają się \n",
    "do ulepszenia ewolucyjnego:\n",
    "</p>\n",
    "\n",
    "1. Kształt sieci neuronowej\n",
    "2. Wagi połączeń międzyneuronowych\n",
    "3. Dane przechowywane w pamięci agentów\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Po dokładnym przeanalizowaniu powyższych możliwości zdecydowaliśmy się odrzucić opcje numer 1 oraz 3. \n",
    "O wyeliminowaniu manipulacji kształtem sieci neuronowej zadecydował fakt, że wolimy skupić się na\n",
    "stworzeniu jednego wariantu sieci i jego dogłębnej analizie niż na pobieżnym tworzeniu wielu modeli tylko \n",
    "po to, żeby różniły się budową. Natomiast jeśli chodzi o opcję trzecią, to jej odrzucenie wynika z \n",
    "natury DQN, której agent przechowuje w pamięci jedynie mały  fragment z całego zbioru danych, \n",
    "na którym był uczony. W takim wypadku używanie go jako podstawy do dalszego ulepszania najprawdopodobniej nie \n",
    "prowadziłoby do uzyskania wymiernych korzyści. W ten sposób stwierdziliśmy, że najciekawszym i (oby) \n",
    "najefektywniejszym podejściem będzie zastosowanie algorytmu genetycznego operującego na wagach połączeń \n",
    "międzyneuronowych.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Etapy działania\n",
    "Podsumowując przeprowadzoną powyżej analizę, zdecydowaliśmy się podzielić realizację projektu na następujące \n",
    "fazy:\n",
    "1. Implementacja i gromadzenie rezultatów naiwnego agenta, który podejmuje decyzje w sposób losowy. Posłuży on \n",
    "jako wyznacznik do porównywania innych modeli.\n",
    "2. Implementacja i gromadzenie rezultatów agenta wykorzystującego prostą sieć neuronową.\n",
    "3. Modyfikacja implementacji agenta oraz parametrów sieci neuronowej w celu poprawy osiąganych rezultatów.\n",
    "4. Wytrenowanie początkowej populacji agentów i zastosowanie algorytmu genetycznego do stworzenia osobnika z \n",
    "jak najlepiej dostosowanymi wagami międzyneuronowymi.\n",
    "5. Porównanie osiągniętych rezultatów i wyciągnięcie wniosków z doświadczenia.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ocena jakości modelu\n",
    "<p style='text-align: justify;'>\n",
    "Aby rzetelnie ocenić jakość modelu, należy najpierw upewnić się, że wyniki przez niego osiągane są stabilne oraz \n",
    "wiarygodne. Mianem stabilnych określamy je wtedy, gdy w trakcie rozgrywania kolejnych gier przyjmują one zbliżone \n",
    "do siebie wartości. Aby osiągnąć taki rezultat, wymagane jest trenowanie modelu na odpowiedniej \n",
    "(nie za małej) liczbie gier. Metodą prób i błędów określiliśmy, że dla różnych modeli w naszym środowisku treningowym\n",
    "liczba ta oscyluje przeważnie w granicach 200.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Z kolei jeśli chodzi o wiarygodność, to uzyskiwane przez dany model rezultaty uznajemy za wiarygodne wtedy, gdy \n",
    "nie są one wypaczone przez losowe warunki początkowe. Istnieją dwa różne podejścia pomagające w zapewnieniu \n",
    "wiarygodności wyników:\n",
    "</p>\n",
    "\n",
    "1. Trenowanie każdego modeli w identycznych warunków początkowych\n",
    "2. Zastosowanie pewnej funkcji oceny, np. wyciągnięcia średniej z kilku różnych prób wytrenowania tego samego modelu.\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Ze względu na to, że ideą głębokiego Q-learningu jest eksploracja środowiska w sposób losowy, nasz wybór musiał \n",
    "paść na opcję numer 2.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Zważając na powyższe rozważania zdecydowaliśmy, że każdy z naszych modeli będzie trenowany dokładnie cztery \n",
    "razy po dwieście gier. Z rezultatów osiąganych we wszystkich czterech podejściach wyciągniemy średnią, a \n",
    "następnie porównamy ją z innymi modelami.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zastosowane technologie\n",
    "<p style='text-align: justify;'>\n",
    "Kod projektu napisaliśmy w języku Python w wersji 3.6. Sieć neuronowa oraz wszelkie struktury i metody\n",
    "z nią  związane pochodzą z wysokopoziomowego API Kerasa bazującego na bibliotece TensorFlow. Jeśli zaś \n",
    "chodzi o samo środowisko do trenowania agenta, wykorzystaliśmy zestaw narzędzi AI Gym, a dokładniej\n",
    "grę Cart Pole. Ponadto, aby maksymalnie wykorzystać możliwości sprzętowe zdecydowaliśmy, że \n",
    "części kodu wymagające wykonywania sekwencyjnego będą uruchamiane z użyciem procesora, a fragmenty \n",
    "możliwe do zrównoleglenia obsłuży karta graficzna. \n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dygresja na temat podjętych decyzji\n",
    "<p style='text-align: justify;'>\n",
    "Pierwszą wersję modelu stworzyliśmy z myślą o uniwersalności. Zasadniczo była ona zdolna do współpracy z \n",
    "większością środowisk dostępnych poprzez interfejs AI Gym. Dzięki temu zabiegowi byliśmy w stanie przeprowadzić \n",
    "serię prób pozwalających na wyłonienie najlepszego kandydata do dalszego ulepszania modelu. Testy zaczęliśmy od \n",
    "bardziej złożonych środowisk (BipedalWalker-v2 - kombinacje 8 różnych możliwych do podjęcia akcji) poprzez te średnio \n",
    "skomplikowane (LunarLander-v2 - możliwe do wykonania 4 akce) aż po najłatwiejsze (CartPole-v1 - tylko 2 możliwe akcje). \n",
    "Niestety okazało się, że nasz sprzęt nie jest w stanie wytrenować sieci neuronowej dla najtrudniejszego \n",
    "środowiska w racjonalnych ramach czasowych, więc postanowiliśmy skupić się na grze o średniej truności - Lunar Lander.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Mówiąc konkretniej, trenowanie naszego modelu do stopnia, w którym jego wyniki są powtarzalne i pozwalają na \n",
    "wyciąganie sensownych wniosków, zajmuje około 5 minut w najprostszym środowisku (CartPole), w drugim \n",
    "co do złożoności LunarLander czas ten jest około 3 krotnie większy, natomiast w BipedalWalker jest aż 10 krotnie większy. \n",
    "Idąc dalej, po oszacowaniu, że do zakończenia projektu będziemy musieli trenować model w różnych konfiguracjach blisko \n",
    "100 razy, najtrudniejszym środowiskiem na jakie mogliśmy sobie pozwolić był Lunar Lander.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Opis środowiska LunarLander-v2\n",
    "<p style='text-align: justify;'>\n",
    "Lądowisko ma zawsze współrzędne (0,0). Współrzędne to dwie pierwsze liczby w wektorze stanu. Nagroda za \n",
    "przebycie drogi od góry ekranu aż do lądowiska i bez użycia silników to około 100-140 punktów. \n",
    "Gra kończy się w momencie, gdy lądownik rozbija się lub ląduje, otrzymując dodatkowe -100 lub +100 punktów. Każda noga,\n",
    "która styka się z ziemią to dodatkowe +10 punktów. Uruchamianie głównego silinika to -0.3 punkta na klatkę. Skuteczne \n",
    "lądowanie to 200 punktów. Lądowanie poza lądowiskiem jest możliwe. Paliwo jest nieskończone, więc agent może najpierw \n",
    "nauczyć się latać,a potem wylądować za pierwszym razem. Możliwe do wykonania 4 dyskretne akcje: nie rób nic, uruchom \n",
    "lewy silnik, uruchom główny silnik, uruchom prawy silnik przyjmujące kolejno wartości 0, 1, 2 oraz 3.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <center>Część praktyczna</center>\n",
    "## Preludium\n",
    "<p style='text-align: justify;'>\n",
    "W poniższych podrozdziałach zawarliśmy przegląd wszystkich wykonanych przez nas modeli. Przedstawione są one w \n",
    "sposób chronologiczny względem ich powstania. Prezentacji podlega zarówno kod źródłowy jak i osiągane wyniki. \n",
    "Pondaty pozwoliliśmy sobie pominąć komentowanie oczywistych części kodu, aby nie zaciemniać tego, \n",
    "co najważniejsze, czyli rezultatów i wniosków z tworzenia kolejnych modeli. Natomiast jeśli chodzi o fragmenty,\n",
    "które uznaliśmy za ważne, to są one opisane bezpośrednio w kodzie lub w kolejnym akapicie pod nim.\n",
    "</p>\n",
    "\n",
    "***\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <b>Naiwny agent losowy</b>\n",
    "Pierwszy zaimplementowany przez nas agent podejmuje decyzje o tym, jaką akcę wykonać, w sposób losowy. Stanowi \n",
    "swego rodzaju obiekt porównawczy dla późniejszych modeli. Jego implementacja wygląda następująco:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import random\n",
    "import gym     # Pełni rolę interfejsu dla środowiska LunarLander"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NUM_OF_AGENTS = 4          # liczba agentów którzy będą wytrenowani od podstaw.\n",
    "NUM_OF_EPISODES = 100      # liczba gier rozegranych przez  jednego agenta.\n",
    "FRAMES_PER_EPISODE = 1000  # liczba klatek równa liczbie akcji do podjęcia przez agenta w każdej grze\n",
    "GAME_ID = \"LunarLander-v2\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "game = gym.make(GAME_ID)             # Tworzymy instancję określonej wcześniej gry\n",
    "num_of_actions = game.action_space.n # wyłuskujemy ilość akcji, które agent ma do wyboru\n",
    "is_done = False\n",
    "avgs = []\n",
    "for model in range(NUM_OF_AGENTS): \n",
    "    scores = []\n",
    "    for episode in range(NUM_OF_EPISODES):\n",
    "        current_state = game.reset()\n",
    "        total_score = 0\n",
    "        for frame in range(FRAMES_PER_EPISODE):\n",
    "            # game.render()\n",
    "            action = random.randrange(num_of_actions) # wybór losowej akcji z zakresu możliwych\n",
    "            new_state, gained_reward, is_done, info = game.step(action) \n",
    "            total_score += gained_reward\n",
    "            if is_done:\n",
    "                break\n",
    "        scores.append(total_score)             # zapamiętujemy wynik po zakończeniu gry\n",
    "    avgs.append(sum(scores)/len(scores)) # zapamiętujemy średni wynik osiągniety przez agenta\n",
    "game.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Do wytrenowania agenta potrzebujemy dwóch metod z interfejsu <b>gym</b> operujących na grze:\n",
    "1. <b>reset()</b> - inicjalizuje nową grę oraz zwraca stan początkowy agenta\n",
    "2. <b>step(action)</b> - wykonuje akcję podaną jako argument, zwraca kolejno: nowy stan, zdobytą nagrodę, flagę wskazującą \n",
    "czy gra się zakończyła oraz obiekt zawierający informacje dla programisty (debug info)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 has avarage: -43.51177257637014\n",
      "Model 1 has avarage: -37.65263149030239\n",
      "Model 2 has avarage: -48.572572568606176\n",
      "Model 3 has avarage: -47.90503956072182\n",
      "Overall avg: -44.41050404900014\n"
     ]
    }
   ],
   "source": [
    "for i, avg in enumerate(avgs): # Wyświetlamy wyniki poszczególnych agentów\n",
    "    print(\"Model {} has avarage: {}\".format(i, avg))\n",
    "print(\"Overall avg: {}\".format(sum(avgs)/len(avgs)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Wnioski\n",
    "<p style='text-align: justify;'>\n",
    "Działania agenta były losowe, więc zgodnie z przewidywaniami osiągane przez niego wyniki były słabe. Poniżej można \n",
    "zobaczyć krótkie nagranie przedstawiające sposób jego działania:\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center><img src=\"https://media.giphy.com/media/jUujItyYyVsMRHulK2/giphy.gif\" width=\"460\" height=\"280\" /></center>\n",
    "\n",
    "***\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agent DQN v1\n",
    "<p style='text-align: justify;'>\n",
    "Następnym krokiem było usprawnienie naiwnego agenta poprzez dodanie prostej sieci neuronowej oraz metod\n",
    "odpowiedzialnych za jej trenowanie. \n",
    "</p>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf # używamy go do określenia, który fragment kodu wykonuje się na GPU"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SimpleDqnNpc:\n",
    "    \"Klasa implementująca agenta DQN opartego o prostą sieć neuronową\"\n",
    "\n",
    "    def __init__(self, num_of_inputs, num_of_outputs):\n",
    "        \"\"\"\n",
    "        num_of_inputs - długość wektora będącego wejściem dla sieci neuronowej\n",
    "        num_of_outputs - ilość wyjść z sieci neuronowej\n",
    "        \"\"\"\n",
    "\n",
    "        self._num_of_inputs = num_of_inputs\n",
    "        self._num_of_outputs = num_of_outputs\n",
    "        self._exploration_rate = 1.0           # Prawdopodobieństwo podjęcia losowej akcji\n",
    "        self._exploration_rate_min = 0.1\n",
    "        self._exploration_rate_decay = 0.997\n",
    "        self.memory = deque(maxlen=1024)\n",
    "        self._init_model()\n",
    "\n",
    "    def _init_model(self):\n",
    "        \"\"\"\n",
    "        Inicjalizuje model sieci neuronowej.\n",
    "        \"\"\"\n",
    "\n",
    "        self._model = Sequential()\n",
    "        self._model.add(Dense(self._num_of_inputs, input_dim=self._num_of_inputs, activation='linear'))\n",
    "        self._model.add(Dense(self._num_of_outputs, activation='linear'))\n",
    "        self._model.compile(optimizer=SGD(), loss='mean_squared_error')\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Przewiduje i zwraca akcję, którą należy wykonać\"\"\"\n",
    "\n",
    "        if np.random.rand() <= self._exploration_rate:\n",
    "            return random.randrange(self._num_of_outputs)\n",
    "        act_values = self._model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def retain(self, current_state, taken_action, gained_reward, future_state, is_lost):\n",
    "        \"\"\"Zapisuje dany przypadek w pamięci agenta\"\"\"\n",
    "\n",
    "        self.memory.append((current_state, taken_action, gained_reward, future_state, is_lost))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"\n",
    "        Doszkala sieć neuronową na losowym fragmencie z jego pamięci\n",
    "        batch-size - rozmiar fragmentu pamięci\n",
    "        \"\"\"\n",
    "\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        for current_state, taken_action, gained_reward, future_state, is_done in batch:\n",
    "            next_act_best_profit = gained_reward\n",
    "            if not is_done:\n",
    "                future_act_profits = self._model.predict(future_state)\n",
    "                next_act_best_profit = np.amax(future_act_profits[0])\n",
    "            current_act_profits = self._model.predict(current_state)\n",
    "            current_act_profits[0][taken_action] = next_act_best_profit\n",
    "            with tf.device('/device:GPU:0'):\n",
    "                self._model.fit(x=current_state, y=current_act_profits, epochs=1, verbose=0)\n",
    "        if self._exploration_rate > self._exploration_rate_min:\n",
    "            self._exploration_rate *= self._exploration_rate_decay\n",
    "        else:\n",
    "            self._exploration_rate = 0.0\n",
    "\n",
    "    def load(self, model_path):\n",
    "        \"\"\"Wczytuje model z pamięci\"\"\"\n",
    "\n",
    "        self._model.load_weights(model_path)\n",
    "\n",
    "    def save(self, model_path):\n",
    "        \"\"\"Zapisuje modele do pamięci\"\"\"\n",
    "\n",
    "        self._model.save_weights(model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> <p style='text-align: justify;'>\n",
    "Sieć neuronowa zaimplementowana w metodzie <b>_init_model()</b> została stworzona jako proste odzwieciedlenie posiadanych\n",
    "przez nas danych, czyli warstwa wejściowa ma ilość neuronów równą rozmiarowi danych wejściowych, a wyjściowa taki jak \n",
    "ilość możliwych do podjęcia akcji. Parametry takie jak funkcja aktywacji czy straty na razie wybraliśmy losowo.\n",
    "<br/>\n",
    "Implementacja metody <b>replay(batch_size)</b> jest jedną z najprostszych powszechnie stosowanych w modelach DQN. \n",
    "Wybiera małą próbkę danych (aby uniknąć znacznego spowolnienia całego procesu), a następnie dotrenowuje na niej model poprzez\n",
    "próbę przewidzenia najlepszej decyzji dla następnego stanu i skojarzenia jej z obecnym stanem. Można sobie to\n",
    "wyobrażać jako coraz dalsze i dalsze spoglądanie w przyszłość. Dzięki zastosowaniu podwójnie zakończonej kolejki\n",
    "(deque) jako pamięci, zyskujemy duże prawdopodobieństwo na kilkukrotne dotrenowanie modelu na danym przypadku, a jednocześnie\n",
    "uniknięcie przetrenowania. Dodatkowo po każdym wywołaniu tej metody obniżamy współczynnik eksploracji agenta, \n",
    "czyli zmniejszamy szansę na wykonanie losowego ruchu.\n",
    "</p>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NUM_OF_AGENTS = 4\n",
    "NUM_OF_EPISODES = 200\n",
    "FRAMES_PER_EPISODE = 1000\n",
    "BATCH_SIZE = 16              # Rozmiar próbki danych do dotrenowywania modelu\n",
    "GAME_ID = \"LunarLander-v2\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "game = gym.make(GAME_ID)\n",
    "num_of_actions = game.action_space.n\n",
    "observation_size = game.observation_space.shape[0]\n",
    "npc = SimpleDqnNpc(observation_size, num_of_actions)\n",
    "is_done = False\n",
    "avgs = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for model in range(NUM_OF_AGENTS):\n",
    "    scores = []\n",
    "    for episode in range(NUM_OF_EPISODES):\n",
    "        total_score = 0\n",
    "        current_state = np.reshape(game.reset(), [1, observation_size])\n",
    "        for frame in range(FRAMES_PER_EPISODE):\n",
    "            # game.render()\n",
    "            action = npc.act(current_state)\n",
    "            new_state, gained_reward, is_done, info = game.step(action)\n",
    "            new_state = np.reshape(new_state, [1, observation_size])\n",
    "            npc.retain(current_state, action, gained_reward, new_state, is_done)\n",
    "            total_score += gained_reward\n",
    "            current_state = new_state\n",
    "            if len(npc.memory) > BATCH_SIZE:\n",
    "                npc.replay(BATCH_SIZE)\n",
    "            if is_done:\n",
    "                break\n",
    "        scores.append(total_score)\n",
    "    npc.save(\"simple_dqn_\" + str(model) + \".h5\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> <p style='text-align: justify;'>\n",
    "Zmiana kształtu zmiennych przechowujących stany wynika z implementacji kerasa. Żeby dane zostały poprawnie \n",
    "zinterpretowane należy umieścić je w zagnieżdżonych listach.\n",
    "</p>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 has avarage: -136.8427404998284\n",
      "Model 1 has avarage: -142.7302459281223\n",
      "Model 2 has avarage: -137.1295521235213\n",
      "Model 3 has avarage: -137.8901023058812\n",
      "Overall avg: -138.6481602143383\n"
     ]
    }
   ],
   "source": [
    "for i, avg in enumerate(avgs):\n",
    "    print(\"Model {} has avarage: {}\".format(i, avg))\n",
    "print(\"Overall avg: {}\".format(sum(avgs) / len(avgs)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Wnioski\n",
    "<p style='text-align: justify;'>\n",
    "Liczbowe wyniki agenta z prostą siecią neuronową są znacznie gorsze od wyników agenta naiwnego. Po dogłębnej analizie\n",
    "wydarzeń oraz zapisu wideo można zauważyć, że agent rzeczywiście początkowo skupia się na eksploracji otoczenia\n",
    "uruchamiając silniki w przypadkowej kolejności. Niemniej jednak wraz z upływem kolejnych prób i zmniejszania się \n",
    "współczynnika eksploracji, zaczyna podejmować najlepsze (według niego) decyzje, czyli po prostu opadać w dół. \n",
    "Poniekąd jest to racjonalna decyzja, bo właśnie na tym polega Lunar Lander, jednake agent nie nauczył się zwracać \n",
    "uwagi na te mniej oczywiste czynniki wspomagające sukcec, takie jak stabilizacja lotu czy niezbaczanie z pionowej\n",
    "trajektorii lotu.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Wynika to z faktu, że kształt i parametry naszej sieci neuronowej nie są zbyt dobrze dobrane. Przede wszystkim \n",
    "liniowa funkcja aktywacji dla obu warstw to marnowanie potencjału wynikającego ze stosowania sieci neuronowej. \n",
    "Jeśli dana warstwa neuronów używa liniowej funkcji aktywacji do określenia pobudzenia poszczególnych neuronów, a \n",
    "następnie przekazuje je jako impulsz do kolejnej warstwy pobudzanej liniowo, to na końcu nie otrzymujemy niczego innego \n",
    "niż po prostu pewną funkcję liniową określoną na podstawie wartości wejść. To oznacza, że wszystkie znajdujące się\n",
    "obok siebie warstwy z liniową funkcją aktywacji mogą po prostu zostać zamienione w jedną warstwę. Innym problemem funkcji\n",
    "liniowej jest to, że w miarę dotrenowywania jej wartości na wyjściu mogą zmierzać do nieskończoności, aż w pewnym\n",
    "momencie przepełnić stos i być interpretowane jako <i>NaN</i>.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style='text-align: justify;'>\n",
    "Drugim, poza funkcją straty, rzucającym się w oczy parametrem jest optymalizator. Użyliśmy algorytmu \n",
    "stochastycznego spadku wzdłuż gradientu, który skupia się na określeniu tendencji optymalizowanej funkcji \n",
    "biorąc pod uwagę jedynie położenie, w którym znajduje się w danym momenice. Jeśli zastanowimy się dłużej \n",
    "nad tym, jaki efekt chcemy osiągnąć w modelu DQN, to dojdziemy do wniosku, że ważne jest dla nas skupianie \n",
    "się nie na jednym, a na kilku krokach naprzód. \n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center><img src=\"https://media.giphy.com/media/JtM8dLWsjSVWYnvMWT/giphy.gif\" width=\"460\" height=\"280\" /></center>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agent DQN v1\n",
    "<p style='text-align: justify;'>\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}